{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-02 11:22:23.379442: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-02 11:22:25.551051: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-02 11:22:25.551088: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-02 11:22:31.025912: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-02 11:22:31.026324: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-02 11:22:31.026344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Dropout, Input, concatenate\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from signal.feather file\n",
    "df_1 = pd.read_feather('/home/brandon/Omdena/Omdena HeartKinetics/HeartKinetics - Project/signals.feather')\n",
    "df_2 = pd.read_feather('/home/brandon/Omdena/Omdena HeartKinetics/HeartKinetics - Project/records.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>0 days 00:00:00</th>\n",
       "      <th>0 days 00:00:00.005000</th>\n",
       "      <th>0 days 00:00:00.010000</th>\n",
       "      <th>0 days 00:00:00.015000</th>\n",
       "      <th>0 days 00:00:00.020000</th>\n",
       "      <th>0 days 00:00:00.025000</th>\n",
       "      <th>0 days 00:00:00.030000</th>\n",
       "      <th>0 days 00:00:00.035000</th>\n",
       "      <th>...</th>\n",
       "      <th>0 days 00:00:59.955000</th>\n",
       "      <th>0 days 00:00:59.960000</th>\n",
       "      <th>0 days 00:00:59.965000</th>\n",
       "      <th>0 days 00:00:59.970000</th>\n",
       "      <th>0 days 00:00:59.975000</th>\n",
       "      <th>0 days 00:00:59.980000</th>\n",
       "      <th>0 days 00:00:59.985000</th>\n",
       "      <th>0 days 00:00:59.990000</th>\n",
       "      <th>0 days 00:00:59.995000</th>\n",
       "      <th>0 days 00:01:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frontiers_CP-66_CP-66_20210716-000000</td>\n",
       "      <td>nrg_lin</td>\n",
       "      <td>1.279607e-10</td>\n",
       "      <td>8.848643e-10</td>\n",
       "      <td>2.081432e-09</td>\n",
       "      <td>3.121196e-09</td>\n",
       "      <td>3.971332e-09</td>\n",
       "      <td>4.824464e-09</td>\n",
       "      <td>5.844231e-09</td>\n",
       "      <td>7.626334e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>3.843697e-08</td>\n",
       "      <td>4.362340e-08</td>\n",
       "      <td>4.407751e-08</td>\n",
       "      <td>4.144375e-08</td>\n",
       "      <td>3.601437e-08</td>\n",
       "      <td>2.377446e-08</td>\n",
       "      <td>1.072449e-08</td>\n",
       "      <td>4.133584e-09</td>\n",
       "      <td>1.771588e-09</td>\n",
       "      <td>2.260352e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frontiers_CP-66_CP-66_20210716-000000</td>\n",
       "      <td>nrg_rot</td>\n",
       "      <td>5.556937e-11</td>\n",
       "      <td>2.217992e-10</td>\n",
       "      <td>3.255192e-10</td>\n",
       "      <td>5.954308e-10</td>\n",
       "      <td>1.205670e-09</td>\n",
       "      <td>1.569920e-09</td>\n",
       "      <td>1.437969e-09</td>\n",
       "      <td>1.264825e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>7.596838e-10</td>\n",
       "      <td>1.185676e-09</td>\n",
       "      <td>1.408344e-09</td>\n",
       "      <td>9.396106e-10</td>\n",
       "      <td>3.263996e-10</td>\n",
       "      <td>8.116416e-10</td>\n",
       "      <td>8.458283e-10</td>\n",
       "      <td>1.133015e-09</td>\n",
       "      <td>1.393282e-09</td>\n",
       "      <td>5.907058e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frontiers_CP-66_CP-66_20210716-000000</td>\n",
       "      <td>pwr_lin</td>\n",
       "      <td>-2.655856e-09</td>\n",
       "      <td>-1.179579e-08</td>\n",
       "      <td>-3.378029e-08</td>\n",
       "      <td>-2.047574e-08</td>\n",
       "      <td>7.866821e-08</td>\n",
       "      <td>1.499196e-07</td>\n",
       "      <td>3.369719e-07</td>\n",
       "      <td>6.925233e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.326742e-06</td>\n",
       "      <td>4.334578e-07</td>\n",
       "      <td>-5.060552e-07</td>\n",
       "      <td>-8.149085e-07</td>\n",
       "      <td>-1.776411e-06</td>\n",
       "      <td>-2.887063e-06</td>\n",
       "      <td>-2.095681e-06</td>\n",
       "      <td>-8.204266e-07</td>\n",
       "      <td>-1.517324e-07</td>\n",
       "      <td>3.144448e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frontiers_CP-66_CP-66_20210716-000000</td>\n",
       "      <td>pwr_rot</td>\n",
       "      <td>-7.878670e-08</td>\n",
       "      <td>-2.514187e-07</td>\n",
       "      <td>-1.262197e-07</td>\n",
       "      <td>-1.130888e-07</td>\n",
       "      <td>-2.835921e-07</td>\n",
       "      <td>-3.726526e-07</td>\n",
       "      <td>-3.114805e-07</td>\n",
       "      <td>-3.268763e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>7.192581e-08</td>\n",
       "      <td>7.764132e-08</td>\n",
       "      <td>-1.740640e-08</td>\n",
       "      <td>-1.393403e-07</td>\n",
       "      <td>-1.414921e-08</td>\n",
       "      <td>9.265420e-08</td>\n",
       "      <td>-4.781753e-08</td>\n",
       "      <td>1.226706e-07</td>\n",
       "      <td>-8.519107e-08</td>\n",
       "      <td>-1.559372e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frontiers_CP-66_CP-66_20210716-000000</td>\n",
       "      <td>rsp</td>\n",
       "      <td>-4.379060e-01</td>\n",
       "      <td>-4.374503e-01</td>\n",
       "      <td>-4.365397e-01</td>\n",
       "      <td>-4.352323e-01</td>\n",
       "      <td>-4.334734e-01</td>\n",
       "      <td>-4.313219e-01</td>\n",
       "      <td>-4.287250e-01</td>\n",
       "      <td>-4.257420e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>7.997676e-01</td>\n",
       "      <td>7.925788e-01</td>\n",
       "      <td>7.853790e-01</td>\n",
       "      <td>7.781603e-01</td>\n",
       "      <td>7.709196e-01</td>\n",
       "      <td>7.636476e-01</td>\n",
       "      <td>7.563429e-01</td>\n",
       "      <td>7.489946e-01</td>\n",
       "      <td>7.416031e-01</td>\n",
       "      <td>7.341561e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 12003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               record_id  channel  0 days 00:00:00  \\\n",
       "0  Frontiers_CP-66_CP-66_20210716-000000  nrg_lin     1.279607e-10   \n",
       "1  Frontiers_CP-66_CP-66_20210716-000000  nrg_rot     5.556937e-11   \n",
       "2  Frontiers_CP-66_CP-66_20210716-000000  pwr_lin    -2.655856e-09   \n",
       "3  Frontiers_CP-66_CP-66_20210716-000000  pwr_rot    -7.878670e-08   \n",
       "4  Frontiers_CP-66_CP-66_20210716-000000      rsp    -4.379060e-01   \n",
       "\n",
       "   0 days 00:00:00.005000  0 days 00:00:00.010000  0 days 00:00:00.015000  \\\n",
       "0            8.848643e-10            2.081432e-09            3.121196e-09   \n",
       "1            2.217992e-10            3.255192e-10            5.954308e-10   \n",
       "2           -1.179579e-08           -3.378029e-08           -2.047574e-08   \n",
       "3           -2.514187e-07           -1.262197e-07           -1.130888e-07   \n",
       "4           -4.374503e-01           -4.365397e-01           -4.352323e-01   \n",
       "\n",
       "   0 days 00:00:00.020000  0 days 00:00:00.025000  0 days 00:00:00.030000  \\\n",
       "0            3.971332e-09            4.824464e-09            5.844231e-09   \n",
       "1            1.205670e-09            1.569920e-09            1.437969e-09   \n",
       "2            7.866821e-08            1.499196e-07            3.369719e-07   \n",
       "3           -2.835921e-07           -3.726526e-07           -3.114805e-07   \n",
       "4           -4.334734e-01           -4.313219e-01           -4.287250e-01   \n",
       "\n",
       "   0 days 00:00:00.035000  ...  0 days 00:00:59.955000  \\\n",
       "0            7.626334e-09  ...            3.843697e-08   \n",
       "1            1.264825e-09  ...            7.596838e-10   \n",
       "2            6.925233e-07  ...            1.326742e-06   \n",
       "3           -3.268763e-07  ...            7.192581e-08   \n",
       "4           -4.257420e-01  ...            7.997676e-01   \n",
       "\n",
       "   0 days 00:00:59.960000  0 days 00:00:59.965000  0 days 00:00:59.970000  \\\n",
       "0            4.362340e-08            4.407751e-08            4.144375e-08   \n",
       "1            1.185676e-09            1.408344e-09            9.396106e-10   \n",
       "2            4.334578e-07           -5.060552e-07           -8.149085e-07   \n",
       "3            7.764132e-08           -1.740640e-08           -1.393403e-07   \n",
       "4            7.925788e-01            7.853790e-01            7.781603e-01   \n",
       "\n",
       "   0 days 00:00:59.975000  0 days 00:00:59.980000  0 days 00:00:59.985000  \\\n",
       "0            3.601437e-08            2.377446e-08            1.072449e-08   \n",
       "1            3.263996e-10            8.116416e-10            8.458283e-10   \n",
       "2           -1.776411e-06           -2.887063e-06           -2.095681e-06   \n",
       "3           -1.414921e-08            9.265420e-08           -4.781753e-08   \n",
       "4            7.709196e-01            7.636476e-01            7.563429e-01   \n",
       "\n",
       "   0 days 00:00:59.990000  0 days 00:00:59.995000  0 days 00:01:00  \n",
       "0            4.133584e-09            1.771588e-09     2.260352e-09  \n",
       "1            1.133015e-09            1.393282e-09     5.907058e-10  \n",
       "2           -8.204266e-07           -1.517324e-07     3.144448e-07  \n",
       "3            1.226706e-07           -8.519107e-08    -1.559372e-07  \n",
       "4            7.489946e-01            7.416031e-01     7.341561e-01  \n",
       "\n",
       "[5 rows x 12003 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>hf_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frontiers_CP-66_CP-66_20210716-000000</td>\n",
       "      <td>Frontiers</td>\n",
       "      <td>CP-66</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.650</td>\n",
       "      <td>66.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frontiers_CP-59_CP-59_20210716-000000</td>\n",
       "      <td>Frontiers</td>\n",
       "      <td>CP-59</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.580</td>\n",
       "      <td>56.5</td>\n",
       "      <td>57.0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frontiers_CP-61_CP-61_20210716-000000</td>\n",
       "      <td>Frontiers</td>\n",
       "      <td>CP-61</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.640</td>\n",
       "      <td>83.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frontiers_UP-03_UP-03_20180404-000000</td>\n",
       "      <td>Frontiers</td>\n",
       "      <td>UP-03</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.625</td>\n",
       "      <td>66.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frontiers_CP-57_CP-57_20210716-000000</td>\n",
       "      <td>Frontiers</td>\n",
       "      <td>CP-57</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.740</td>\n",
       "      <td>62.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               record_id   study_id subject_id     sex  \\\n",
       "0  Frontiers_CP-66_CP-66_20210716-000000  Frontiers      CP-66    Male   \n",
       "1  Frontiers_CP-59_CP-59_20210716-000000  Frontiers      CP-59  Female   \n",
       "2  Frontiers_CP-61_CP-61_20210716-000000  Frontiers      CP-61  Female   \n",
       "3  Frontiers_UP-03_UP-03_20180404-000000  Frontiers      UP-03    Male   \n",
       "4  Frontiers_CP-57_CP-57_20210716-000000  Frontiers      CP-57    Male   \n",
       "\n",
       "   height  weight   age  hf_type  \n",
       "0   1.650    66.0  59.0  UNKNOWN  \n",
       "1   1.580    56.5  57.0  UNKNOWN  \n",
       "2   1.640    83.0  68.0  UNKNOWN  \n",
       "3   1.625    66.0  96.0  UNKNOWN  \n",
       "4   1.740    62.0  79.0  UNKNOWN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.sample(n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 500 entries, 523 to 3712\n",
      "Columns: 12003 entries, record_id to 0 days 00:01:00\n",
      "dtypes: float64(12001), object(2)\n",
      "memory usage: 45.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_1, df_2, on='record_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>0 days 00:00:00</th>\n",
       "      <th>0 days 00:00:00.005000</th>\n",
       "      <th>0 days 00:00:00.010000</th>\n",
       "      <th>0 days 00:00:00.015000</th>\n",
       "      <th>0 days 00:00:00.020000</th>\n",
       "      <th>0 days 00:00:00.025000</th>\n",
       "      <th>0 days 00:00:00.030000</th>\n",
       "      <th>0 days 00:00:00.035000</th>\n",
       "      <th>...</th>\n",
       "      <th>0 days 00:00:59.990000</th>\n",
       "      <th>0 days 00:00:59.995000</th>\n",
       "      <th>0 days 00:01:00</th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>hf_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAP-PWV_AR_0_20220216-104722_14xreOAagSOvTo</td>\n",
       "      <td>pwr_rot</td>\n",
       "      <td>2.725892e-04</td>\n",
       "      <td>2.600196e-04</td>\n",
       "      <td>-4.677789e-03</td>\n",
       "      <td>-1.442255e-02</td>\n",
       "      <td>-2.304427e-02</td>\n",
       "      <td>-1.823420e-02</td>\n",
       "      <td>-1.231658e-02</td>\n",
       "      <td>-7.787862e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HAP-PWV</td>\n",
       "      <td>AR</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.65</td>\n",
       "      <td>83.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NoHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAP-PWV_AR_0_20220216-104722_14xreOAagSOvTo</td>\n",
       "      <td>pwr_lin</td>\n",
       "      <td>-3.447429e-03</td>\n",
       "      <td>-2.436293e-02</td>\n",
       "      <td>-4.329472e-02</td>\n",
       "      <td>-5.436322e-02</td>\n",
       "      <td>-5.184779e-02</td>\n",
       "      <td>-4.038738e-02</td>\n",
       "      <td>-3.335469e-02</td>\n",
       "      <td>-2.847132e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HAP-PWV</td>\n",
       "      <td>AR</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.65</td>\n",
       "      <td>83.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NoHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLEEP-SIMUL_WN_Centrale_20181116-135820_Dy2O8z...</td>\n",
       "      <td>pwr_rot</td>\n",
       "      <td>3.003260e-22</td>\n",
       "      <td>-1.521833e-07</td>\n",
       "      <td>-4.906979e-07</td>\n",
       "      <td>-5.393484e-07</td>\n",
       "      <td>-1.652647e-07</td>\n",
       "      <td>3.358045e-07</td>\n",
       "      <td>6.957038e-07</td>\n",
       "      <td>6.029253e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.915410e-06</td>\n",
       "      <td>-6.610624e-07</td>\n",
       "      <td>1.350023e-06</td>\n",
       "      <td>SLEEP-SIMUL</td>\n",
       "      <td>WN</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.83</td>\n",
       "      <td>83.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NoHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAP-Test_MC_1_20220214-095918_nWY6q10DoiKSD</td>\n",
       "      <td>pwr_rot</td>\n",
       "      <td>2.122173e-04</td>\n",
       "      <td>3.900320e-04</td>\n",
       "      <td>-2.762427e-03</td>\n",
       "      <td>-9.362706e-03</td>\n",
       "      <td>-1.624580e-02</td>\n",
       "      <td>-1.480981e-02</td>\n",
       "      <td>-1.158942e-02</td>\n",
       "      <td>-8.247929e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HAP-Test</td>\n",
       "      <td>MC</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NoHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SLEEP-SIMUL_PV_Valsalva_20181108-133432_XzE6DB...</td>\n",
       "      <td>pwr_lin</td>\n",
       "      <td>-8.528647e-26</td>\n",
       "      <td>2.438460e-07</td>\n",
       "      <td>1.063671e-06</td>\n",
       "      <td>1.554705e-06</td>\n",
       "      <td>8.642172e-07</td>\n",
       "      <td>-6.867579e-07</td>\n",
       "      <td>-2.304062e-06</td>\n",
       "      <td>-3.017287e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366741e-07</td>\n",
       "      <td>3.263732e-07</td>\n",
       "      <td>3.719346e-07</td>\n",
       "      <td>SLEEP-SIMUL</td>\n",
       "      <td>PV</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.65</td>\n",
       "      <td>65.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NoHF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 12010 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           record_id  channel  \\\n",
       "0        HAP-PWV_AR_0_20220216-104722_14xreOAagSOvTo  pwr_rot   \n",
       "1        HAP-PWV_AR_0_20220216-104722_14xreOAagSOvTo  pwr_lin   \n",
       "2  SLEEP-SIMUL_WN_Centrale_20181116-135820_Dy2O8z...  pwr_rot   \n",
       "3        HAP-Test_MC_1_20220214-095918_nWY6q10DoiKSD  pwr_rot   \n",
       "4  SLEEP-SIMUL_PV_Valsalva_20181108-133432_XzE6DB...  pwr_lin   \n",
       "\n",
       "   0 days 00:00:00  0 days 00:00:00.005000  0 days 00:00:00.010000  \\\n",
       "0     2.725892e-04            2.600196e-04           -4.677789e-03   \n",
       "1    -3.447429e-03           -2.436293e-02           -4.329472e-02   \n",
       "2     3.003260e-22           -1.521833e-07           -4.906979e-07   \n",
       "3     2.122173e-04            3.900320e-04           -2.762427e-03   \n",
       "4    -8.528647e-26            2.438460e-07            1.063671e-06   \n",
       "\n",
       "   0 days 00:00:00.015000  0 days 00:00:00.020000  0 days 00:00:00.025000  \\\n",
       "0           -1.442255e-02           -2.304427e-02           -1.823420e-02   \n",
       "1           -5.436322e-02           -5.184779e-02           -4.038738e-02   \n",
       "2           -5.393484e-07           -1.652647e-07            3.358045e-07   \n",
       "3           -9.362706e-03           -1.624580e-02           -1.480981e-02   \n",
       "4            1.554705e-06            8.642172e-07           -6.867579e-07   \n",
       "\n",
       "   0 days 00:00:00.030000  0 days 00:00:00.035000  ...  \\\n",
       "0           -1.231658e-02           -7.787862e-03  ...   \n",
       "1           -3.335469e-02           -2.847132e-02  ...   \n",
       "2            6.957038e-07            6.029253e-07  ...   \n",
       "3           -1.158942e-02           -8.247929e-03  ...   \n",
       "4           -2.304062e-06           -3.017287e-06  ...   \n",
       "\n",
       "   0 days 00:00:59.990000  0 days 00:00:59.995000  0 days 00:01:00  \\\n",
       "0                     NaN                     NaN              NaN   \n",
       "1                     NaN                     NaN              NaN   \n",
       "2           -2.915410e-06           -6.610624e-07     1.350023e-06   \n",
       "3                     NaN                     NaN              NaN   \n",
       "4            1.366741e-07            3.263732e-07     3.719346e-07   \n",
       "\n",
       "      study_id  subject_id     sex  height  weight   age  hf_type  \n",
       "0      HAP-PWV          AR  Female    1.65    83.0  70.0     NoHF  \n",
       "1      HAP-PWV          AR  Female    1.65    83.0  70.0     NoHF  \n",
       "2  SLEEP-SIMUL          WN  Female    1.83    83.0  22.0     NoHF  \n",
       "3     HAP-Test          MC  Female    1.60    68.0  69.0     NoHF  \n",
       "4  SLEEP-SIMUL          PV  Female    1.65    65.0  21.0     NoHF  \n",
       "\n",
       "[5 rows x 12010 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoHF       263\n",
       "UNKNOWN    169\n",
       "HFrEF       46\n",
       "HFpEF       12\n",
       "HFmEF       10\n",
       "Name: hf_type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['hf_type'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep Learning Classifiers</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Merge your dataframes here\n",
    "merged_df = pd.merge(df_1, df_2, on='record_id')\n",
    "\n",
    "# Generate some example time series data\n",
    "data = merged_df.select_dtypes(include=[np.float64]).values\n",
    "\n",
    "# Reshape the data to fit the input shape of the model\n",
    "num_samples, num_timesteps = data.shape\n",
    "data = data.reshape(num_samples, num_timesteps, 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.0.0 --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-02 11:22:40.263824: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-02 11:22:40.263851: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-02 11:22:40.263870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (brandon-HP-255-G7-Notebook-PC): /proc/driver/nvidia/version does not exist\n",
      "2023-04-02 11:22:40.264092: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 20s 1s/step - loss: nan - accuracy: 0.9450 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 18s 1s/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 18s 1s/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 18s 1s/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 18s 1s/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbdf6fbc0a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping # import EarlyStopping callback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert target variable to binary vector\n",
    "y = np.array(merged_df['hf_type'])\n",
    "y = np.expand_dims(y, axis=1)\n",
    "y = (y == np.arange(2)).astype(int)\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "n_timesteps = data.shape[1]\n",
    "n_features = data.shape[2]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) # changed number of units and activation function\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Add EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Fit the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LSTM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13/13 [==============================] - 453s 34s/step - loss: nan - accuracy: 0.9950 - val_loss: nan - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 462s 36s/step - loss: nan - accuracy: 1.0000 - val_loss: nan - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert target variable to binary vector\n",
    "y = np.array(merged_df['hf_type'])\n",
    "y = np.expand_dims(y, axis=1)\n",
    "y = (y == np.arange(2)).astype(int)\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(n_timesteps, n_features), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(patience=1, monitor='val_loss')\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Multilayer Perceptron</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 1s 23ms/step - loss: 0.7024 - accuracy: 0.5031 - val_loss: 0.7047 - val_accuracy: 0.4875\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6910 - accuracy: 0.5125 - val_loss: 0.7087 - val_accuracy: 0.4500\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6850 - accuracy: 0.5625 - val_loss: 0.7079 - val_accuracy: 0.4875\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6798 - accuracy: 0.5813 - val_loss: 0.7112 - val_accuracy: 0.4750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Generate dummy data\n",
    "n_samples, n_timesteps, n_features = 400, 10, 5\n",
    "X = np.random.rand(n_samples, n_timesteps, n_features)\n",
    "y = np.random.randint(2, size=n_samples)\n",
    "\n",
    "# Flatten the input data\n",
    "X_flat = np.reshape(X, (n_samples, n_timesteps * n_features))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_flat, X_test_flat, y_train, y_test = train_test_split(X_flat, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=n_timesteps * n_features))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(patience=3, monitor='val_loss')\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train_flat, y_train, epochs=50, batch_size=32, validation_data=(X_test_flat, y_test), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feed Forward Neural Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 1s 28ms/step - loss: 0.9497 - accuracy: 0.6475 - val_loss: 0.1087 - val_accuracy: 0.9900\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.2567 - accuracy: 0.9325 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.2065 - accuracy: 0.9725 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.1601 - accuracy: 0.9800 - val_loss: 6.0186e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.1671 - accuracy: 0.9825 - val_loss: 2.3036e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0929 - accuracy: 0.9925 - val_loss: 1.0804e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0423 - accuracy: 0.9900 - val_loss: 6.0306e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0726 - accuracy: 0.9875 - val_loss: 4.1316e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0301 - accuracy: 0.9950 - val_loss: 2.8354e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0147 - accuracy: 0.9950 - val_loss: 1.9353e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.4218e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0178 - accuracy: 0.9950 - val_loss: 9.4176e-06 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0149 - accuracy: 0.9975 - val_loss: 6.7764e-06 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0846 - accuracy: 0.9975 - val_loss: 5.1888e-06 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0048 - accuracy: 0.9975 - val_loss: 4.1781e-06 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.4768e-06 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.0130 - accuracy: 0.9975 - val_loss: 2.8568e-06 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.2394e-06 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.8536e-06 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0069 - accuracy: 0.9975 - val_loss: 1.5385e-06 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.2223e-06 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.0043e-06 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 8.3946e-07 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 7.1840e-07 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 6.1637e-07 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 4.8794e-07 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0046 - accuracy: 0.9975 - val_loss: 3.6190e-07 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0117 - accuracy: 0.9950 - val_loss: 2.8871e-07 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.3770e-07 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 9.5512e-04 - accuracy: 1.0000 - val_loss: 2.1019e-07 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 4.0332e-04 - accuracy: 1.0000 - val_loss: 1.9208e-07 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.6959e-07 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0087 - accuracy: 0.9975 - val_loss: 1.4472e-07 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 8.8637e-04 - accuracy: 1.0000 - val_loss: 1.1895e-07 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.0048 - accuracy: 0.9975 - val_loss: 9.9776e-08 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0044 - accuracy: 0.9975 - val_loss: 9.3901e-08 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 9.8305e-04 - accuracy: 1.0000 - val_loss: 8.3074e-08 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 5.9647e-08 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 5.4890e-04 - accuracy: 1.0000 - val_loss: 5.0109e-08 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 4.9657e-04 - accuracy: 1.0000 - val_loss: 4.4990e-08 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.0201 - accuracy: 0.9975 - val_loss: 4.1516e-08 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0036 - accuracy: 0.9975 - val_loss: 2.6014e-08 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 3.1652e-04 - accuracy: 1.0000 - val_loss: 2.1228e-08 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 8.4165e-04 - accuracy: 1.0000 - val_loss: 1.8734e-08 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0150 - accuracy: 0.9975 - val_loss: 1.4714e-08 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.3291e-08 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 1.5662e-04 - accuracy: 1.0000 - val_loss: 1.2298e-08 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0050 - accuracy: 0.9975 - val_loss: 1.0358e-08 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 5.8636e-04 - accuracy: 1.0000 - val_loss: 7.8739e-09 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 2.4599e-04 - accuracy: 1.0000 - val_loss: 9.1668e-09 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Convert non-numeric values to NaN\n",
    "merged_df = merged_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Replace NaN with a specific value (e.g., 0)\n",
    "merged_df = merged_df.fillna(0)\n",
    "\n",
    "# Split data into features and target\n",
    "X = merged_df.drop(['hf_type'], axis=1).values.astype('float32')\n",
    "y = merged_df['hf_type'].values\n",
    "\n",
    "# Convert target variable to binary vector\n",
    "y = (y == 1).astype(int)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create FNN model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(patience=5, monitor='val_loss')\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision Tree Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "F1 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define features and target variable\n",
    "features = [\"age\", \"weight\", \"height\"]\n",
    "target = \"hf_type\"\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    merged_df[features], merged_df[target], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate f1 score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"F1 score:\", f1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Random Forest Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a random dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_classes=2, random_state=42)\n",
    "\n",
    "# Reshape y to a 1D array\n",
    "y = np.ravel(y)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a random forest classifier with 100 trees\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "# Print the accuracy of the classifier\n",
    "print(f\"Accuracy: {clf.score(X_test.reshape(X_test.shape[0], -1), y_test)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install streamlit --quiet\n",
    "!pip install cachetools --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting cachetools==5.3.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: cachetools\n",
      "  Attempting uninstall: cachetools\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: cachetools 4.2.4\n",
      "    Uninstalling cachetools-4.2.4:\n",
      "      Successfully uninstalled cachetools-4.2.4\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-auth 2.3.3 requires cachetools<5.0,>=2.0.0, but you have cachetools 5.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cachetools-5.3.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install cachetools==5.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import streamlit as st\n",
    "\n",
    "# Define features and target variable\n",
    "features = [\"age\", \"weight\", \"height\"]\n",
    "target = \"hf_type\"\n",
    "\n",
    "# Initialize classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the app\n",
    "def app():\n",
    "    # Add Omdena logo\n",
    "    st.image(\"omdena_logo.png\")\n",
    "\n",
    "    # Add a title\n",
    "    st.title(\"Heart Failure Risk Detector\")\n",
    "\n",
    "    # Add input fields for age, weight, and height\n",
    "    age = st.number_input(\"Enter age\", value=40, min_value=0, max_value=120)\n",
    "    weight = st.number_input(\"Enter weight in kg\", value=70, min_value=0, max_value=500)\n",
    "    height = st.number_input(\"Enter height in cm\", value=170, min_value=0, max_value=300)\n",
    "\n",
    "    # Create a dataframe with the user's input\n",
    "    data = pd.DataFrame({\n",
    "        \"age\": [age],\n",
    "        \"weight\": [weight],\n",
    "        \"height\": [height]\n",
    "    })\n",
    "\n",
    "    # Make a prediction using the classifier\n",
    "    prediction = clf.predict(data)\n",
    "\n",
    "    # Add a button to make the prediction\n",
    "    if st.button(\"Predict\"):\n",
    "        # Show the prediction\n",
    "        if prediction == 0:\n",
    "            st.error(\"You are at risk of heart failure!\")\n",
    "        else:\n",
    "            st.success(\"You are not at risk of heart failure.\")\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit run app.py\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Experimental Dashboard w/Decision Tree</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember to replace the path with your own path and feel free to change the name of the dashboard.html to a style of your choice\n",
    "\n",
    "from flask import Flask, render_template, request\n",
    "from wtforms import Form, StringField, validators\n",
    "import pandas as pd\n",
    "\n",
    "# Define the Flask app\n",
    "app = Flask(__name__, template_folder='/home/brandon/Omdena/Omdena HeartKinetics/HeartKinetics - Project/Dataset1/templates')\n",
    "\n",
    "\n",
    "# Define the input form\n",
    "class InputForm(Form):\n",
    "    input_data = StringField('Input Data', [validators.DataRequired()])\n",
    "\n",
    "# Define the dashboard route\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def dashboard():\n",
    "    form = InputForm(request.form)\n",
    "    result = None\n",
    "    if request.method == 'POST' and form.validate():\n",
    "        # Get the input data from the form\n",
    "        input_data = form.input_data.data\n",
    "        \n",
    "        # Perform some processing on the input data\n",
    "        processed_data = input_data.upper()\n",
    "        \n",
    "        # Create a DataFrame from the processed data\n",
    "        df = pd.DataFrame({'Processed Data': [processed_data]})\n",
    "        \n",
    "        # Set the DataFrame as the result\n",
    "        result = df.to_html(index=False)\n",
    "        \n",
    "    return render_template('dashboard.html', form=form, result=result)\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5001)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "551d2f2aa75f5805e59d733f97e947ecf028f9238130be675818dc07c0576ec0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
