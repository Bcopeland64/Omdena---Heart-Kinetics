{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pytorch_tabnet in /home/brandon/anaconda3/lib/python3.9/site-packages (4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /home/brandon/anaconda3/lib/python3.9/site-packages (from pytorch_tabnet) (1.23.5)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /home/brandon/anaconda3/lib/python3.9/site-packages (from pytorch_tabnet) (1.2.2)\n",
      "Requirement already satisfied: scipy>1.4 in /home/brandon/anaconda3/lib/python3.9/site-packages (from pytorch_tabnet) (1.9.1)\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /home/brandon/anaconda3/lib/python3.9/site-packages/torch-1.12.1-py3.9-linux-x86_64.egg (from pytorch_tabnet) (1.12.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /home/brandon/anaconda3/lib/python3.9/site-packages (from pytorch_tabnet) (4.64.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/brandon/anaconda3/lib/python3.9/site-packages (from scikit_learn>0.21->pytorch_tabnet) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/brandon/anaconda3/lib/python3.9/site-packages (from scikit_learn>0.21->pytorch_tabnet) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/brandon/anaconda3/lib/python3.9/site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (4.4.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/brandon/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pytorch_tabnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 20:27:14.482106: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-27 20:27:14.632965: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-27 20:27:14.632987: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-27 20:27:15.441752: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-27 20:27:15.441825: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-27 20:27:15.441833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_61669/972074598.py\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     def __init__(self, conv1d_filters=64, conv1d_kernel_size=3, conv2d_filters=16, \n",
      "\u001b[0;32m/tmp/ipykernel_61669/972074598.py\u001b[0m in \u001b[0;36mNeuralNetwork\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m     \"\"\"\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# make predictions using the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# calculate performance metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.callbacks import EarlyStopping\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "from pytorch_tabnet.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class NeuralNetwork(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, conv1d_filters=64, conv1d_kernel_size=3, conv2d_filters=16, \n",
    "                 conv2d_kernel_size=(3, 3), tabnet_feature_dim=32, tabnet_num_attention_heads=4,\n",
    "                 tabnet_num_decision_steps=4, tabnet_virtual_batch_size=64, tabnet_batch_size=1024,\n",
    "                 tabnet_dropout=0.2, conv1d_epochs=50, conv1d_patience=10, conv2d_epochs=50,\n",
    "                 conv2d_patience=10, tabnet_epochs=50, tabnet_patience=10, num_classes=2):\n",
    "        self.conv1d_filters = conv1d_filters\n",
    "        self.conv1d_kernel_size = conv1d_kernel_size\n",
    "        self.conv2d_filters = conv2d_filters\n",
    "        self.conv2d_kernel_size = conv2d_kernel_size\n",
    "        self.tabnet_feature_dim = tabnet_feature_dim\n",
    "        self.tabnet_num_attention_heads = tabnet_num_attention_heads\n",
    "        self.tabnet_num_decision_steps = tabnet_num_decision_steps\n",
    "        self.tabnet_virtual_batch_size = tabnet_virtual_batch_size\n",
    "        self.tabnet_batch_size = tabnet_batch_size\n",
    "        self.tabnet_dropout = tabnet_dropout\n",
    "        self.conv1d_epochs = conv1d_epochs\n",
    "        self.conv1d_patience = conv1d_patience\n",
    "        self.conv2d_epochs = conv2d_epochs\n",
    "        self.conv2d_patience = conv2d_patience\n",
    "        self.tabnet_epochs = tabnet_epochs\n",
    "        self.tabnet_patience = tabnet_patience\n",
    "        self.num_classes = num_classes\n",
    "        self.keras_model = None\n",
    "        self.pytorch_model = None\n",
    "        self.tabnet_model = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # 1D Convolutional Neural Network\n",
    "        self.keras_model = Sequential()\n",
    "        self.keras_model.add(Conv1D(self.conv1d_filters, self.conv1d_kernel_size, activation='relu', input_shape=(X.shape[1], 1)))\n",
    "        self.keras_model.add(MaxPooling1D(pool_size=2))\n",
    "        self.keras_model.add(Conv1D(self.conv1d_filters*2, self.conv1d_kernel_size, activation='relu'))\n",
    "        self.keras_model.add(MaxPooling1D(pool_size=2))\n",
    "        self.keras_model.add(Flatten())\n",
    "        self.keras_model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        self.keras_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        X_reshape = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "        y_categorical = to_categorical(y)\n",
    "        self.keras_model.fit(X_reshape, y_categorical, epochs=self.conv1d_epochs, \n",
    "                             validation_split=0.2, callbacks=[EarlyStopping(patience=self.conv1d_patience)])\n",
    "        \n",
    "        # 2D Convolutional Neural Network\n",
    "        self.pytorch_model = CNN2D(X.shape[1], self.conv2d_filters, self.num_classes, kernel_size=self.conv2d_kernel_size)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.pytorch_model.parameters())\n",
    "        early_stop = EarlyStopping(patience=self.conv2d_patience)\n",
    "        X_reshape = np.reshape(X, (X.shape[0], 1, X.shape[1], 1))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_reshape, y, test_size=0.2, random_state=42)\n",
    "        train_dataset = CustomDataset(X_train, y_train)\n",
    "        val_dataset = CustomDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "        train_losses, val_losses = train_model(self.pytorch_model, train_loader, val_loader, criterion, optimizer, \n",
    "                                                self.conv2d_epochs, early_stop)\n",
    "        \n",
    "        # TabNet Classifier\n",
    "        self.tabnet_model = TabNetClassifier(\n",
    "            n_d=self.tabnet_feature_dim, \n",
    "            n_a=self.tabnet_num_attention_heads,\n",
    "            n_steps=self.tabnet_num_decision_steps,\n",
    "            gamma=1.5,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            seed=42,\n",
    "            momentum=0.02,\n",
    "            clip_value=2.,\n",
    "            lambda_sparse=0.,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=2e-2),\n",
    "            mask_type=TabNetMaskType.entmax,\n",
    "            scheduler_params=dict(mode=\"min\",\n",
    "                                  patience=self.tabnet_patience,\n",
    "                                  min_lr=1e-5,\n",
    "                                  factor=0.5,),\n",
    "            scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            verbose=0\n",
    "        )\n",
    "                                      \n",
    "        # fitting the tabnet model\n",
    "        self.tabnet_model.fit(X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val,\n",
    "                              patience=self.tabnet_patience, batch_size=self.tabnet_batch_size, \n",
    "                              virtual_batch_size=self.tabnet_virtual_batch_size, \n",
    "                              num_workers=self.tabnet_num_workers, drop_last=False)\n",
    "        \n",
    "        print(\"Training completed.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the output for input X\n",
    "        \n",
    "        Args:\n",
    "        - X: input data\n",
    "        \n",
    "        Returns:\n",
    "        - y_pred: predicted labels\n",
    "        \"\"\"\n",
    "        X_reshape = np.reshape(X, (X.shape[0], 1, X.shape[1], 1))\n",
    "        y_pred_cnn1d = self.keras_model.predict(X_reshape)\n",
    "        y_pred_cnn2d = self.pytorch_model.predict(torch.tensor(X_reshape).float())\n",
    "        y_pred_tabnet = self.tabnet_model.predict(X)\n",
    "        y_pred = np.mean([y_pred_cnn1d, y_pred_cnn2d, y_pred_tabnet], axis=0)\n",
    "        return np.round(y_pred)\n",
    "    \n",
    "    # create a pipeline object\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('nn', NeuralNetwork())])\n",
    "\n",
    "    \n",
    "    def evaluate_pipeline(X_test, y_test, pipeline):\n",
    "     \"\"\"\n",
    "        Evaluate the pipeline on the test set and display performance metrics\n",
    "    \n",
    "        Args:\n",
    "        - X_test (numpy array): test set input\n",
    "        - y_test (numpy array): test set target\n",
    "        - pipeline (Pipeline): trained pipeline\n",
    "    \n",
    "        Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    # make predictions using the pipeline\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # display performance metrics\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    print(f'Precision: {precision:.3f}')\n",
    "    print(f'Recall: {recall:.3f}')\n",
    "    print(f'F1-score: {f1:.3f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `predict()` method takes input `X`, reshapes it, and predicts the output labels for each of the three models. The final prediction is the average of the predictions from each model. The output is rounded to obtain binary labels.\n",
    "\n",
    "Now we will define a `Pipeline` class that will allow us to run the entire data preprocessing and model training process in a single step. The `Pipeline` class will have the following methods:\n",
    "- `__init__()`: constructor method that initializes the parameters and the models\n",
    "- `fit()`: fits the model on the input data\n",
    "- `predict()`: predicts the output for input data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "def to_path(p: Union[str, Path]) -> Path:\n",
    "    return p if isinstance(p, Path) else Path(p)\n",
    "\n",
    "def find_records(path: str):\n",
    "    search_path: str = f\"{path}/**/signals/\"\n",
    "    all_paths = list(map(lambda x: str(to_path(x).parent), glob.glob(search_path, recursive=True)))\n",
    "    return all_paths\n",
    "\n",
    "class RecordReader():\n",
    "    def __init__(self, path: Union[str, Path]):\n",
    "        self.path = to_path(path)\n",
    "\n",
    "    def load_signal(self, sig_name):\n",
    "        return xr.open_zarr(self.path / \"signals\" / sig_name / \"dataset\")\n",
    "\n",
    "    def load_signal_meta(self, sig_name):\n",
    "        with open(self.path / \"signals\" / sig_name / \"meta.json\", \"r\") as meta:\n",
    "            return json.load(meta)\n",
    "    \n",
    "    def load_metadata(self):\n",
    "        with open(self.path / \"meta.json\", \"r\") as meta:\n",
    "            return json.load(meta)\n",
    "\n",
    "    def load_crf_metadata(self):\n",
    "        with open(self.path / \"crf.json\", \"r\") as meta:\n",
    "            return json.load(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = find_records((\"./\"))\n",
    "print(records, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for r in records:\n",
    "    reader = RecordReader(r)\n",
    "    metadata = reader.load_metadata()\n",
    "    scg_metadata = reader.load_signal_meta('scg-k')\n",
    "    rsp_metadata = reader.load_signal_meta('rsp')\n",
    "    crf_data = reader.load_crf_metadata()\n",
    "    \n",
    "    value = {\n",
    "            'age': metadata['subject']['age']['value'],\n",
    "            'sex' : metadata['subject']['sex'],\n",
    "            'weight': metadata['subject']['weight']['value'],\n",
    "            'height' : metadata['subject']['height']['value'],\n",
    "            'subject_id' : crf_data['subject_id'],\n",
    "            'study_id' : crf_data['study_id'],\n",
    "            'hf_type' : crf_data['hf_type'],\n",
    "            'sample_rate_scgk' : scg_metadata['sample_rate'],\n",
    "            'nrg_lin_scgk' : reader.load_signal(\"scg-k\").nrg.sel(motion=\"lin\").to_pandas(),\n",
    "            'nrg_rot_scgk' : reader.load_signal(\"scg-k\").nrg.sel(motion=\"rot\").to_pandas(),\n",
    "            'pwr_lin_scgk': reader.load_signal(\"scg-k\").pwr.sel(motion=\"lin\").to_pandas(),\n",
    "            'pwr_rot_scgk': reader.load_signal(\"scg-k\").pwr.sel(motion=\"rot\").to_pandas(),\n",
    "            'sample_rate_rsp' : rsp_metadata['sample_rate'],\n",
    "            'rsp': reader.load_signal(\"rsp\").signal.to_pandas()\n",
    "            }\n",
    "    data[metadata['id']] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pywt\n",
    "\n",
    "def calculate_features(ts):\n",
    "    mean = np.mean(ts)\n",
    "    std = np.std(ts)\n",
    "    median = np.median(ts)\n",
    "    minimum = np.min(ts)\n",
    "    maximum = np.max(ts)\n",
    "    skewness = skew(ts)\n",
    "    kurt = kurtosis(ts)\n",
    "    rms = np.sqrt(np.mean(np.square(ts)))\n",
    "    zero_crossings = np.sum(np.diff(np.sign(ts)) != 0)\n",
    "    \n",
    "    # Wavelet transformation\n",
    "    wavelet = 'db4'\n",
    "    coeffs = pywt.wavedec(ts, wavelet, level=4)\n",
    "    \n",
    "    # Calculate wavelet features\n",
    "    wavelet_mean = np.mean(np.concatenate(coeffs))\n",
    "    wavelet_std = np.std(np.concatenate(coeffs))\n",
    "    wavelet_energy = np.sum(np.square(np.concatenate(coeffs)))\n",
    "    \n",
    "    return [mean, std, median, minimum, maximum, skewness, kurt, rms, zero_crossings, wavelet_mean, wavelet_std, wavelet_energy]\n",
    "\n",
    "# Compute time series features for each subject\n",
    "for subject_id, value in data.items():\n",
    "    for feature in ['nrg_lin_scgk', 'nrg_rot_scgk', 'pwr_lin_scgk', 'pwr_rot_scgk', 'rsp']:\n",
    "        ts = value[feature].values\n",
    "        features = calculate_features(ts)\n",
    "        \n",
    "        # Store the computed features\n",
    "        value[f\"{feature}_mean\"] = features[0]\n",
    "        value[f\"{feature}_std\"] = features[1]\n",
    "        value[f\"{feature}_median\"] = features[2]\n",
    "        value[f\"{feature}_min\"] = features[3]\n",
    "        value[f\"{feature}_max\"] = features[4]\n",
    "        value[f\"{feature}_skew\"] = features[5]\n",
    "        value[f\"{feature}_kurt\"] = features[6]\n",
    "        value[f\"{feature}_rms\"] = features[7]\n",
    "        value[f\"{feature}_zero_crossings\"] = features[8]\n",
    "        value[f\"{feature}_wavelet_mean\"] = features[9]\n",
    "        value[f\"{feature}_wavelet_std\"] = features[10]\n",
    "        value[f\"{feature}_wavelet_energy\"] = features[11]\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking target\n",
    "df.hf_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the hf_type = UNKNOWN\n",
    "df = df[df[\"hf_type\"]!=\"UNKNOWN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check counts again\n",
    "df.hf_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Encode the hf_type column as integer labels\n",
    "encoder = LabelEncoder()\n",
    "df['hf_type'] = encoder.fit_transform(df['hf_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to code HFpEF and HFmEF into one category, NoHF second category, and HFrEF third\n",
    "def convert(df):\n",
    "    if df[\"hf_type\"]==3:\n",
    "        return 0\n",
    "    elif df[\"hf_type\"]==2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hf_type\"] = df.apply(lambda df: convert(df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import periodogram\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def spectral_entropy(pxx):\n",
    "    psd_norm = pxx / np.sum(pxx)\n",
    "    return entropy(psd_norm)\n",
    "\n",
    "# Initialize a list to store the feature data\n",
    "feature_data = []\n",
    "\n",
    "# Iterate through the data dictionary\n",
    "for key, value in data.items():\n",
    "    # Calculate the periodogram for each time series\n",
    "    freq_nrg_lin, pxx_nrg_lin = periodogram(value['nrg_lin_scgk'])\n",
    "    freq_nrg_rot, pxx_nrg_rot = periodogram(value['nrg_rot_scgk'])\n",
    "    freq_pwr_lin, pxx_pwr_lin = periodogram(value['pwr_lin_scgk'])\n",
    "    freq_pwr_rot, pxx_pwr_rot = periodogram(value['pwr_rot_scgk'])\n",
    "    freq_rsp, pxx_rsp = periodogram(value['rsp'])\n",
    "\n",
    "    # Calculate the frequency-domain features for each time series\n",
    "    features = {\n",
    "        'record_id': key,\n",
    "        'nrg_lin_mean_freq': np.mean(freq_nrg_lin),\n",
    "        'nrg_lin_median_freq': np.median(freq_nrg_lin),\n",
    "        'nrg_lin_peak_freq': freq_nrg_lin[np.argmax(pxx_nrg_lin)],\n",
    "        'nrg_lin_spectral_entropy': spectral_entropy(pxx_nrg_lin),\n",
    "        'nrg_rot_mean_freq': np.mean(freq_nrg_rot),\n",
    "        'nrg_rot_median_freq': np.median(freq_nrg_rot),\n",
    "        'nrg_rot_peak_freq': freq_nrg_rot[np.argmax(pxx_nrg_rot)],\n",
    "        'nrg_rot_spectral_entropy': spectral_entropy(pxx_nrg_rot),\n",
    "        'pwr_lin_mean_freq': np.mean(freq_pwr_lin),\n",
    "        'pwr_lin_median_freq': np.median(freq_pwr_lin),\n",
    "        'pwr_lin_peak_freq': freq_pwr_lin[np.argmax(pxx_pwr_lin)],\n",
    "        'pwr_lin_spectral_entropy': spectral_entropy(pxx_pwr_lin),\n",
    "        'pwr_rot_mean_freq': np.mean(freq_pwr_rot),\n",
    "        'pwr_rot_median_freq': np.median(freq_pwr_rot),\n",
    "        'pwr_rot_peak_freq': freq_pwr_rot[np.argmax(pxx_pwr_rot)],\n",
    "        'pwr_rot_spectral_entropy': spectral_entropy(pxx_pwr_rot),\n",
    "        'rsp_mean_freq': np.mean(freq_rsp),\n",
    "        'rsp_median_freq': np.median(freq_rsp),\n",
    "        'rsp_peak_freq': freq_rsp[np.argmax(pxx_rsp)],\n",
    "        'rsp_spectral_entropy': spectral_entropy(pxx_rsp)\n",
    "    }\n",
    "    \n",
    "    # Add the features to the feature_data list\n",
    "    feature_data.append(features)\n",
    "\n",
    "# Convert the feature_data list into a DataFrame\n",
    "features_df = pd.DataFrame(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index of features_df to be the record_id\n",
    "features_df.set_index('record_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the main DataFrame with the features_df\n",
    "combined_df = pd.merge(df, features_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.dtypes.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for modeling\n",
    "combined_df.drop(columns=['nrg_lin_scgk', 'nrg_rot_scgk', 'pwr_lin_scgk', 'pwr_rot_scgk', 'rsp'], inplace=True)\n",
    "skim(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate each signal\n",
    "nrg_signal = combined_df[['nrg_lin_mean_freq', 'nrg_lin_median_freq', 'nrg_lin_peak_freq', 'nrg_lin_spectral_entropy', 'nrg_rot_mean_freq', 'nrg_rot_median_freq', 'nrg_rot_peak_freq', 'nrg_rot_spectral_entropy']]\n",
    "rot_signal = combined_df[['pwr_lin_mean_freq', 'pwr_lin_median_freq', 'pwr_lin_peak_freq', 'pwr_lin_spectral_entropy', 'pwr_rot_mean_freq', 'pwr_rot_median_freq', 'pwr_rot_peak_freq', 'pwr_rot_spectral_entropy']]\n",
    "pwr_signal = combined_df[['rsp_mean_freq', 'rsp_median_freq', 'rsp_peak_freq', 'rsp_spectral_entropy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nrg_signal.shape)\n",
    "print(rot_signal.shape)\n",
    "print(pwr_signal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrg_signal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a smaller sample to save memory\n",
    "sample = combined_df.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into X and y\n",
    "X = sample.drop('hf_type', axis=1)  # Drop the 'target' column to get the feature matrix\n",
    "y = sample['hf_type']  # Extract the 'target' column to get the target vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the data for the pipeline\n",
    "fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
